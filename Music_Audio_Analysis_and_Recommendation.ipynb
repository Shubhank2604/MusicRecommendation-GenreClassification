{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv5Y23IKqFva"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import math\n",
        "import librosa\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/genres\"\n",
        "JSON_PATH = \"data_10.json\"\n",
        "SAMPLE_RATE = 22050\n",
        "TRACK_DURATION = 30 # measured in seconds\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
        "\n",
        "\n",
        "def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
        "    \"\"\"\n",
        "    Extracts MFCCs from music dataset and saves them into a json file along witgh genre labels.\n",
        "        :param dataset_path (str): Path to dataset\n",
        "        :param json_path (str): Path to json file used to save MFCCs\n",
        "        :param num_mfcc (int): Number of coefficients to extract\n",
        "        :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n",
        "        :param hop_length (int): Sliding window for FFT. Measured in # of samples\n",
        "        :param: num_segments (int): Number of segments we want to divide sample tracks into\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    # dictionary to store mapping, labels, and MFCCs\n",
        "    data = {\n",
        "        \"mapping\": [],\n",
        "        \"labels\": [],\n",
        "        \"mfcc\": []\n",
        "    }\n",
        "\n",
        "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
        "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
        "\n",
        "    # loop through all genre sub-folder\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
        "\n",
        "        # ensure we're processing a genre sub-folder level\n",
        "        if dirpath is not dataset_path:\n",
        "\n",
        "            # save genre label (i.e., sub-folder name) in the mapping\n",
        "            semantic_label = dirpath.split(\"/\")[-1]\n",
        "            data[\"mapping\"].append(semantic_label)\n",
        "            print(\"\\nProcessing: {}\".format(semantic_label))\n",
        "\n",
        "            # process all audio files in genre sub-dir\n",
        "            for f in filenames:\n",
        "\n",
        "        # load audio file\n",
        "                file_path = os.path.join(dirpath, f)\n",
        "                signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "                # process all segments of audio file\n",
        "                for d in range(num_segments):\n",
        "\n",
        "                    # calculate start and finish sample for current segment\n",
        "                    start = samples_per_segment * d\n",
        "                    finish = start + samples_per_segment\n",
        "\n",
        "                    # extract mfcc\n",
        "                    mfcc = librosa.feature.mfcc(signal[start:finish], sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
        "                    mfcc = mfcc.T\n",
        "\n",
        "                    # store only mfcc feature with expected number of vectors\n",
        "                    if len(mfcc) == num_mfcc_vectors_per_segment:\n",
        "                        data[\"mfcc\"].append(mfcc.tolist())\n",
        "                        data[\"labels\"].append(i-1)\n",
        "                        print(\"{}, segment:{}\".format(file_path, d+1))\n",
        "\n",
        "    # save MFCCs to json file\n",
        "    with open(json_path, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    save_mfcc(DATASET_PATH, JSON_PATH, num_segments=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NOJ0ibRWH4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74a97da-f9e3-4d73-f938-6210e3beb1d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "V8NGlRZeq16n",
        "outputId": "e3af2fec-532c-400b-cbf3-da82cb456e05"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# path to json file that stores MFCCs and genre labels for each processed segment\n",
        "DATA_PATH = \"/content/data_10.json\"\n",
        "\n",
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    # convert lists to numpy arrays\n",
        "    X = np.array(data[\"mfcc\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "\n",
        "    print(\"Data succesfully loaded!\")\n",
        "\n",
        "    return  X, y\n",
        "\n",
        "def plot_history(history):\n",
        "\n",
        "      fig, axs = plt.subplots(2)\n",
        "\n",
        "      # create accuracy sublpot\n",
        "      axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "      axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "      axs[0].set_ylabel(\"Accuracy\")\n",
        "      axs[0].legend(loc=\"lower right\")\n",
        "      axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "      # create error sublpot\n",
        "      axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "      axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "      axs[1].set_ylabel(\"Error\")\n",
        "      axs[1].set_xlabel(\"Epoch\")\n",
        "      axs[1].legend(loc=\"upper right\")\n",
        "      axs[1].set_title(\"Error eval\")\n",
        "\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(DATA_PATH)\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "    # build network topology\n",
        "    model = keras.Sequential([\n",
        "\n",
        "        # input layer\n",
        "        keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),\n",
        "\n",
        "        # 1st dense layer\n",
        "        keras.layers.Dense(512, activation='relu'),\n",
        "\n",
        "        # 2nd dense layer\n",
        "        keras.layers.Dense(256, activation='relu'),\n",
        "\n",
        "        # 3rd dense layer\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "\n",
        "        # output layer\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # compile model\n",
        "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimiser,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=50)\n",
        "    plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data succesfully loaded!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-af53685865c6>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# create train/test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# build network topology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2562\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2563\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2564\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2236\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2237\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2238\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F233XkfcbzTM"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# path to json file that stores MFCCs and genre labels for each processed segment\n",
        "DATA_PATH = \"/content/data_10.json\"\n",
        "\n",
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    # convert lists to numpy arrays\n",
        "    X = np.array(data[\"mfcc\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "\n",
        "    print(\"Data succesfully loaded!\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(DATA_PATH)\n",
        "\n",
        "    # create train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "    # build network topology\n",
        "    model = keras.Sequential([\n",
        "\n",
        "        # input layer\n",
        "        keras.layers.Flatten(input_shape=(X.shape[1], X.shape[2])),\n",
        "\n",
        "        # 1st dense layer\n",
        "        keras.layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        keras.layers.Dropout(0.3),\n",
        "\n",
        "        # 2nd dense layer\n",
        "        keras.layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        keras.layers.Dropout(0.3),\n",
        "\n",
        "        # 3rd dense layer\n",
        "        keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "        keras.layers.Dropout(0.3),\n",
        "\n",
        "        # output layer\n",
        "        keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # compile model\n",
        "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimiser,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=100)\n",
        "\n",
        "    # plot accuracy and error as a function of the epochs\n",
        "    plot_history(history)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WLfdcoddOmT"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.keras as keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_PATH = \"/content/data_10.json\"\n",
        "\n",
        "\n",
        "def load_data(data_path):\n",
        "    \"\"\"Loads training dataset from json file.\n",
        "        :param data_path (str): Path to json file containing data\n",
        "        :return X (ndarray): Inputs\n",
        "        :return y (ndarray): Targets\n",
        "    \"\"\"\n",
        "\n",
        "    with open(data_path, \"r\") as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    X = np.array(data[\"mfcc\"])\n",
        "    y = np.array(data[\"labels\"])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    \"\"\"Plots accuracy/loss for training/validation set as a function of the epochs\n",
        "        :param history: Training history of model\n",
        "        :return:\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(2)\n",
        "\n",
        "    # create accuracy sublpot\n",
        "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
        "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
        "    axs[0].set_ylabel(\"Accuracy\")\n",
        "    axs[0].legend(loc=\"lower right\")\n",
        "    axs[0].set_title(\"Accuracy eval\")\n",
        "\n",
        "    # create error sublpot\n",
        "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
        "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
        "    axs[1].set_ylabel(\"Error\")\n",
        "    axs[1].set_xlabel(\"Epoch\")\n",
        "    axs[1].legend(loc=\"upper right\")\n",
        "    axs[1].set_title(\"Error eval\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "    \"\"\"Loads data and splits it into train, validation and test sets.\n",
        "    :param test_size (float): Value in [0, 1] indicating percentage of data set to allocate to test split\n",
        "    :param validation_size (float): Value in [0, 1] indicating percentage of train set to allocate to validation split\n",
        "    :return X_train (ndarray): Input training set\n",
        "    :return X_validation (ndarray): Input validation set\n",
        "    :return X_test (ndarray): Input test set\n",
        "    :return y_train (ndarray): Target training set\n",
        "    :return y_validation (ndarray): Target validation set\n",
        "    :return y_test (ndarray): Target test set\n",
        "    \"\"\"\n",
        "\n",
        "    # load data\n",
        "    X, y = load_data(DATA_PATH)\n",
        "\n",
        "    # create train, validation and test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "    # add an axis to input sets\n",
        "    X_train = X_train[..., np.newaxis]\n",
        "    X_validation = X_validation[..., np.newaxis]\n",
        "    X_test = X_test[..., np.newaxis]\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "\n",
        "def build_model(input_shape):\n",
        "    \"\"\"Generates CNN model\n",
        "    :param input_shape (tuple): Shape of input set\n",
        "    :return model: CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    # build network topology\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # 1st conv layer\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # 2nd conv layer\n",
        "    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # 3rd conv layer\n",
        "    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu'))\n",
        "    model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "    # flatten output and feed it into dense layer\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.3))\n",
        "\n",
        "    # output layer\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, X, y):\n",
        "    \"\"\"Predict a single sample using the trained model\n",
        "    :param model: Trained classifier\n",
        "    :param X: Input data\n",
        "    :param y (int): Target\n",
        "    \"\"\"\n",
        "    # add a dimension to input data for sample - model.predict() expects a 4d array in this case\n",
        "    X = X[np.newaxis, ...] # array shape (1, 130, 13, 1)\n",
        "\n",
        "    # perform prediction\n",
        "    prediction = model.predict(X)\n",
        "\n",
        "    # get index with max value\n",
        "    predicted_index = np.argmax(prediction, axis=1)\n",
        "\n",
        "    print(\"Target: {}, Predicted label: {}\".format(y, predicted_index))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # get train, validation, test splits\n",
        "    X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "    # create network\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
        "    model = build_model(input_shape)\n",
        "\n",
        "    # compile model\n",
        "    optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimiser,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    model.summary()\n",
        "    # train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_validation, y_validation), batch_size=32, epochs=40)\n",
        "\n",
        "    # plot accuracy/error for training and validation\n",
        "    plot_history(history)\n",
        "\n",
        "    # evaluate model on test set\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "    print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "    # pick a sample to predict from the test set\n",
        "    X_to_predict = X_test[100]\n",
        "    y_to_predict = y_test[100]\n",
        "\n",
        "    # predict sample\n",
        "    predict(model, X_to_predict, y_to_predict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtqRn6dqenHb"
      },
      "source": [
        "X_to_predict_2 = X_test[69]\n",
        "y_to_predict_2 = y_test[69]\n",
        "\n",
        "# predict sample\n",
        "predict(model, X_to_predict_2, y_to_predict_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD22pufp40KD"
      },
      "source": [
        "X_u = X\n",
        "X_u = X_u.reshape(X.shape[0],X.shape[1],X.shape[2],1)\n",
        "X_u.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icQL8o0oxykV"
      },
      "source": [
        "predictions_array = np.zeros(shape=(X.shape[0],1,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isAT9wVWOdGj"
      },
      "source": [
        "X_r = X_u[4]\n",
        "X_r = (X_r)[np.newaxis, ...]\n",
        "X_r.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmhM1A7vw145"
      },
      "source": [
        "for i in range(X.shape[0]):\n",
        "  print(\"Processed \"+str(i))\n",
        "  X_row = X_u[i]\n",
        "  X_row = (X_row)[np.newaxis, ...] # array shape (1, 130, 13, 1)\n",
        "  predictions_array[i] = model.predict(X_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q-7DTJ6eTfe"
      },
      "source": [
        "genre_dict = {0:'blues', 1:'classical', 2:'country', 3:'disco', 4:'hiphop', 5:'jazz', 6:'metal', 7:'pop', 8:'reggae', 9:'rock'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7X5sIrw1jGu"
      },
      "source": [
        "while(True):\n",
        "  print(\"Genres :\")\n",
        "  print()\n",
        "  print(\"blues : 0\")\n",
        "  print(\"classical : 1\")\n",
        "  print(\"country : 2\")\n",
        "  print(\"disco : 3\")\n",
        "  print(\"hiphop : 4\")\n",
        "  print(\"jazz : 5\")\n",
        "  print(\"metal : 6\")\n",
        "  print(\"pop : 7\")\n",
        "  print(\"reggae : 8\")\n",
        "  print(\"rock : 9\")\n",
        "  print()\n",
        "  print()\n",
        "  x = int(input(\"Select your song's genre (0-9) : \"))\n",
        "  s_n = int(input(\"Select the song number of the song you like (0-99) from \" + str(genre_dict[x]) +\" genre and I will recommend you top 5 similar songs, you might like : \"))\n",
        "  d = x*1000 + s_n*10 + 5\n",
        "  ex_song = predictions_array[d]\n",
        "  print()\n",
        "  print(\"Your song's genre is : \")\n",
        "  print()\n",
        "  print(genre_dict[y[d]])\n",
        "  print()\n",
        "  print(\"and song number :\")\n",
        "  print()\n",
        "  print(math.floor(((d%1000)/10)))\n",
        "  min_list = []\n",
        "  min_index_list = []\n",
        "  min = 10000\n",
        "  for i in range(9996):\n",
        "    if (i!=d):\n",
        "      s=np.linalg.norm(predictions_array[i][0] - ex_song[0])\n",
        "\n",
        "    if(s<min):\n",
        "      min = s\n",
        "      min_list.append(s)\n",
        "      min_index_list.append(i)\n",
        "  print()\n",
        "  print(\"The top 5 songs I found most similar to the given song are : \")\n",
        "  print()\n",
        "  res = []\n",
        "  #print(min_index_list[0])\n",
        "  for i in min_index_list:\n",
        "      if i not in res:\n",
        "          res.append(i)\n",
        "  c=-1\n",
        "  for i in range(5):\n",
        "    genre_name = genre_dict[int(res[c]/1000)]\n",
        "    song_no = math.floor(((res[c]%1000)/10))\n",
        "    print(genre_name+\" song number \"+str(song_no))\n",
        "    c = c-1\n",
        "  print()\n",
        "  print()\n",
        "  ans = str(input(print(\"Do you wish to enter more : ? (y/n)\")))\n",
        "  if(ans=='y'):\n",
        "    continue\n",
        "  else:\n",
        "    break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKzAUhZXxLm_"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}